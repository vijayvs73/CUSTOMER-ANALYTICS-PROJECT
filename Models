# STEP 7: Feature Selection (RandomForest, AdaBoost, XGBoost)
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

target_column = "Churn"

# Encode target
y_raw = df[target_column]
if y_raw.dtype == "object":
    le = LabelEncoder()
    y = le.fit_transform(y_raw)
else:
    y = y_raw.astype(int)

# Features
X = df.drop(columns=[target_column])
X = pd.get_dummies(X, drop_first=True)

# Random Forest
rf = RandomForestClassifier()
rf.fit(X, y)
print("Top 5 Features (RandomForest):\n", pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False).head())

from sklearn.impute import SimpleImputer
import pandas as pd
from sklearn.ensemble import AdaBoostClassifier

# Create an imputer to handle missing values
imputer = SimpleImputer(strategy='mean')  # You can choose other strategies like 'median', 'most_frequent', etc.

# Fit and transform the data to handle missing values
X_imputed = imputer.fit_transform(X)

# Now we can use X_imputed in the AdaBoost model
ada = AdaBoostClassifier()
ada.fit(X_imputed, y)
print("Top 5 Features (AdaBoost):\n", pd.Series(ada.feature_importances_, index=X.columns).sort_values(ascending=False).head())

# XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X, y)
print("Top 5 Features (XGBoost):\n", pd.Series(xgb.feature_importances_, index=X.columns).sort_values(ascending=False).head())

